Testar métodos de classificação para dataset COM e SEM alterações após análise;

Outliers e fit dist.
No entanto, como as vars são categoricas não faze sentido fazer fit destr. 

FAzer modelos;

Verificação dos modelos: 
Gráfico ROC 

Decision Trees:
Description: Decision Trees inherently handle categorical data. They work by recursively splitting the data based on the feature that results in the maximum information gain (or other criteria).
Popular Algorithms: CART (Classification and Regression Trees), ID3, C4.5.

Random Forest:
Description: An ensemble method that creates multiple decision trees during training and outputs the class that is the mode of the classes for classification.
Library in Python: RandomForestClassifier in sklearn.ensemble.

Gradient Boosting Machines:
Description: Boosting algorithms train a sequence of weak learners (typically decision trees) in which each subsequent tree corrects the errors of its predecessor.
Popular Algorithms: XGBoost, LightGBM, CatBoost.
Notably, CatBoost is optimized for categorical variables and doesn't require explicit one-hot or label encoding.
Library in Python: XGBClassifier in xgboost, LGBMClassifier in lightgbm, CatBoostClassifier in catboost.
Naive Bayes:

Description: Based on Bayes' theorem, it assumes independence between predictors. For categorical data, the multinomial and Bernoulli variants of Naive Bayes can be used.
Library in Python: MultinomialNB and BernoulliNB in sklearn.naive_bayes.
K-Nearest Neighbors (K-NN):

Description: K-NN can be used with categorical data if you use a distance metric suitable for categorical data, such as the Hamming distance.
Library in Python: KNeighborsClassifier in sklearn.neighbors.
Neural Networks:

Description: Neural networks can handle categorical data, especially when using embeddings for high cardinality categorical variables. This approach is popular in deep learning frameworks.
Library in Python: tensorflow and keras.
Logistic Regression:

Description: While logistic regression is designed for numerical input data, you can use one-hot encoding or dummy variables to convert categorical data into a numerical format and then apply logistic regression.
Library in Python: LogisticRegression in sklearn.linear_model.
Preprocessing Note: For many of these algorithms, you'll need to encode categorical variables into a numerical format. Common methods include one-hot encoding, label encoding, and binary encoding. However, some algorithms (like CatBoost or decision trees in certain implementations) can handle categorical data directly.

Choosing the "best" algorithm often requires experimentation. It's a good idea to start with a simple model to establish a baseline and then experiment with more complex models or algorithms. Hyperparameter tuning and proper validation (using techniques like cross-validation) are also crucial to achieving optimal performance.